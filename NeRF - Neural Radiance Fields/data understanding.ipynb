{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the input datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field of view in X direction - train: 0.6911112070083618\n",
      "Number of frames - train: 100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "jsonTrainFile = \"Data/ship/transforms_train.json\"\n",
    "\n",
    "with open(jsonTrainFile, \"r\") as fp:\n",
    "    jsonTrainData = json.load(fp)\n",
    "\n",
    "print(f\"Field of view in X direction - train: {jsonTrainData['camera_angle_x']}\")\n",
    "print(f\"Number of frames - train: {len(jsonTrainData['frames'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.65905853e-02  1.75296515e-01 -9.83412623e-01 -3.96426296e+00]\n",
      " [-9.98914063e-01 -8.17604642e-03  4.58675772e-02  1.84898108e-01]\n",
      " [-9.31322575e-10  9.84481692e-01  1.75487086e-01  7.07411051e-01]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "./train/r_0\n",
      "Rotation 0.012566370614359171\n"
     ]
    }
   ],
   "source": [
    "first_frame = jsonTrainData[\"frames\"][0]\n",
    "\n",
    "# transformation matrix and related image\n",
    "transform_matix = np.array(first_frame[\"transform_matrix\"])\n",
    "file_name = first_frame[\"file_path\"]\n",
    "\n",
    "print(transform_matix)  # This the Camera-World transform matrix for the respective image.\n",
    "print(file_name)  # Respective image file path.\n",
    "print(f\"Rotation {first_frame['rotation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread('Data/ship/train/r_0.png')\n",
    "cv2.imshow('image', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "## Rays in Computer Graphics\n",
    "\n",
    "    r(t) = O + t.d\n",
    "\n",
    "Here r(t) represent the ray.\n",
    "\n",
    "- O: Origin vector of the ray (starting point)\n",
    "- d: Direction unit vector of the ray\n",
    "- t: Parameter for the ray propagation (like time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to do the further processing in our code, we need to first find the corresponding camera coordinate frame x, y, z values to the image pixels. This is where the computer graphics knowledge comes to play.\n",
    "\n",
    "<center><image src=\"./imgs/image_plane.png\" width=\"500px\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply basic trigonometry, to find a relationship between xi, yi values and Xc, Yc values in terms of the f value(we should know this. This is the focal length of the camera)\n",
    "\n",
    "\n",
    "<center><image src=\"./imgs/camera_coords.png\" width=\"100\"/></center>\n",
    "\n",
    "In above eqation we assume images start the origin from the top left corner, hence the ox, oy values. With this we can get the Camera Origin vector. But getting the ray origin direction vector is bit more complex.\n",
    "\n",
    "Theory behind this is pretty simple and [This artical](https://pyimagesearch.com/2021/11/10/computer-graphics-and-deep-learning-with-nerf-using-tensorflow-and-keras-part-1/) have nice simple explanation about it.\n",
    "\n",
    "But basically what we do is, for a point P in world coordinate system (in homogenius form vector) we need to convert it to camera coordinate frame (This vector can be used to generate the image pixels using the previouly mentioned formula, but in this case we start from there and go backward to get the world coordinates). To do that transformation we need to have `Camera Extrinsic Matrix` for the Camera coordinate system. (Inverse of this matrix is the one given with all the frames in above dataset, so we can directly convert camera coordinates to world coordinates.)\n",
    "\n",
    "<center><image src=\"./imgs/coor_transform.png\" width=\"500\"/></center>\n",
    "\n",
    "Here Xw means world coordinates. Xc means Camera world coordinates. Cex means the Camera Extrinsic parameter matrix. This matrix include the transformations required to make the world coordinates convert to camera coordinates. Using this formula we can calculate the world coordinate of a pixel, given its camera coordinate and using that world coordinate vector we can get the directional unit vector.\n",
    "\n",
    "And also origin of ray is of cause, the translational vector of the world coordinate.\n",
    "\n",
    "Lets look at basic example on above concepts using our training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68acd5d746db9e112a7343296bb3423d1ae6da35b5d50d333630681f8a968c1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
